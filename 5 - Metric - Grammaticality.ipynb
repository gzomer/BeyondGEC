{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "juvenile-driving",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import os\n",
    "\n",
    "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
    "\n",
    "BASE_DIR = '' # Working directory\n",
    "DATA_DIR = f'{BASE_DIR}data/' # Data directory\n",
    "MODELS_DIR = f'{BASE_DIR}models/' # Models directory\n",
    "\n",
    "# Download the file (if we haven't already)\n",
    "if not os.path.exists(f'{BASE_DIR}/data/cola_public_1.1.zip'):\n",
    "    wget.download(url, f'{BASE_DIR}/data/cola_public_1.1.zip')\n",
    "    \n",
    "if not os.path.exists(f'{BASE_DIR}/data/cola_public/'):\n",
    "    !unzip /user/HS229/gz00109/data/cola_public_1.1.zip -d /user/HS229/gz00109/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-sleeve",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "enclosed-table",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 8,551\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_source</th>\n",
       "      <th>label</th>\n",
       "      <th>label_notes</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7838</th>\n",
       "      <td>ad03</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>He kicked him</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8033</th>\n",
       "      <td>ad03</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Euclid was interested in Plato's description o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4879</th>\n",
       "      <td>ks08</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>has no relative pronoun at all.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>r-67</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>You're going to hurt yourself one of these days.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2122</th>\n",
       "      <td>rhl07</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I sent the salesman to the devil.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6608</th>\n",
       "      <td>g_81</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Smith loaned, and his widow later donated, a v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4157</th>\n",
       "      <td>ks08</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>Fifteen dollars in a week are not much.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4508</th>\n",
       "      <td>ks08</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mary sang a song, but Lee did not.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8238</th>\n",
       "      <td>ad03</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Look after yourself!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2942</th>\n",
       "      <td>l-93</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Donna fixed a sandwich for me.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_source  label label_notes  \\\n",
       "7838            ad03      1         NaN   \n",
       "8033            ad03      1         NaN   \n",
       "4879            ks08      1         NaN   \n",
       "1253            r-67      1         NaN   \n",
       "2122           rhl07      1         NaN   \n",
       "6608            g_81      1         NaN   \n",
       "4157            ks08      0           *   \n",
       "4508            ks08      1         NaN   \n",
       "8238            ad03      1         NaN   \n",
       "2942            l-93      1         NaN   \n",
       "\n",
       "                                               sentence  \n",
       "7838                                      He kicked him  \n",
       "8033  Euclid was interested in Plato's description o...  \n",
       "4879                    has no relative pronoun at all.  \n",
       "1253   You're going to hurt yourself one of these days.  \n",
       "2122                  I sent the salesman to the devil.  \n",
       "6608  Smith loaned, and his widow later donated, a v...  \n",
       "4157            Fifteen dollars in a week are not much.  \n",
       "4508                 Mary sang a song, but Lee did not.  \n",
       "8238                               Look after yourself!  \n",
       "2942                     Donna fixed a sandwich for me.  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df = pd.read_csv(f\"{DATA_DIR}/cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "soviet-walker",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(df.sentence.values)\n",
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-trinidad",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "french-equity",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForSequenceClassification, AdamW\n",
    "roberta_base = RobertaForSequenceClassification.from_pretrained('roberta-base').cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-commerce",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "desperate-briefs",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizerFast\n",
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "current-tradition",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dict = tokenizer(sentences, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "broadband-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColaDataset(Dataset):\n",
    "    def __init__(self, encodings, labels, limit=None):\n",
    "        self.encodings = {key: encodings[key][:limit] for key in encodings}\n",
    "        self.labels = labels[:limit]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx], device=device) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "cola_dataset = ColaDataset(encoded_dict, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-lover",
   "metadata": {},
   "source": [
    "## Train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "junior-savage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6,840 training samples\n",
      "  855 validation samples\n",
      "  856 test samples\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * len(cola_dataset))\n",
    "val_size = int(0.1 * len(cola_dataset))\n",
    "test_size = len(cola_dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(cola_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))\n",
    "print('{:>5,} test samples'.format(test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "banner-furniture",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "interpreted-optimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler = RandomSampler(train_dataset),\n",
    "    batch_size=batch_size \n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler = SequentialSampler(val_dataset),\n",
    "    batch_size=batch_size \n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    sampler = SequentialSampler(test_dataset),\n",
    "    batch_size=batch_size \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-baking",
   "metadata": {},
   "source": [
    "# Prepare training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cordless-chance",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "\n",
      "  | Name      | Type                             | Params\n",
      "---------------------------------------------------------------\n",
      "0 | model     | RobertaForSequenceClassification | 124 M \n",
      "1 | valid_acc | Accuracy                         | 0     \n",
      "2 | train_acc | Accuracy                         | 0     \n",
      "---------------------------------------------------------------\n",
      "124 M     Trainable params\n",
      "0         Non-trainable params\n",
      "124 M     Total params\n",
      "498.589   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c3c899b71046469f85c5b7fb75ae1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning.metrics import functional as FM\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "class LMClassifier(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, model, tokenizer, labels):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.labels = labels\n",
    "        self.valid_acc = pl.metrics.Accuracy()\n",
    "        self.train_acc = pl.metrics.Accuracy()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.model.eval()\n",
    "        self.model.cuda()\n",
    "        \n",
    "        input_ids = tokenizer.encode(x, return_tensors='pt').to(device)\n",
    "        outputs = self.model(input_ids)\n",
    "        prob = F.softmax(outputs.logits.detach(), dim=1).cpu().numpy()[0].tolist()\n",
    "        \n",
    "        return {label: prob[index]  for index, label in enumerate(self.labels)}\n",
    "            \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        labels = batch[\"labels\"]\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        token_type_ids = None\n",
    "\n",
    "        outputs = self.model(**batch)\n",
    "        \n",
    "        labels_hat = torch.argmax(outputs.logits, dim=1)        \n",
    "        \n",
    "        self.log('train_loss', outputs.loss)        \n",
    "        self.train_acc(labels_hat, batch['labels'])\n",
    "        self.log('train_acc', self.train_acc, on_epoch=True, prog_bar=True)  \n",
    "        \n",
    "        return outputs.loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):  \n",
    "        outputs = self.model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"labels\"]\n",
    "        )\n",
    "            \n",
    "        labels_hat = torch.argmax(outputs.logits, dim=1)\n",
    "        \n",
    "        val_acc = self.valid_acc(labels_hat, batch['labels'])\n",
    "        self.log('valid_acc', val_acc, on_step=True, on_epoch=True, prog_bar=True)    \n",
    " \n",
    "    \n",
    "    def test_step(self, batch, batch_idx):   \n",
    "        outputs = self.model(input_ids=batch[\"input_ids\"])\n",
    "        \n",
    "        labels_hat = torch.argmax(outputs.logits, dim=1)\n",
    "        acc = FM.accuracy(labels_hat, batch['labels'])\n",
    "\n",
    "        metrics = {'test_acc': acc}\n",
    "        self.log_dict(metrics)\n",
    "        return metrics\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.model.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "        return optimizer\n",
    "    \n",
    "cola_model = LMClassifier(roberta_base, tokenizer, ['wrong', 'correct'])\n",
    "\n",
    "tb_logger = pl_loggers.TensorBoardLogger('cola-logs/')\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=5, checkpoint_callback=False, logger=tb_logger)\n",
    "trainer.fit(cola_model, train_dataloader, validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cooperative-quantum",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "/user/HS229/gz00109/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66376d985a16487096075adab4effcfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.8333333134651184}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_acc': 0.8333333134651184}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cola_model = LMClassifier(roberta_base, tokenizer, ['wrong', 'correct'])\n",
    "test_trainer = pl.Trainer(gpus=1)\n",
    "test_trainer.test(cola_model, test_dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "lined-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_base.save_pretrained(MODELS_DIR+'roberta-cola-v1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
