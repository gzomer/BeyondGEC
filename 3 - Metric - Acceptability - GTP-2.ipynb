{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '' # Working dir\n",
    "DATA_DIR = f'{BASE_DIR}data/'\n",
    "MODELS_DIR = f'{BASE_DIR}models/'\n",
    "EXPACE_DIR = f'{DATA_DIR}/expace-v1/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-weekend",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-microwave",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-strand",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "device = 'cuda'\n",
    "model_id = 'gpt2'\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-walter",
   "metadata": {},
   "source": [
    "# Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-swing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import string\n",
    "\n",
    "def read_file(file):\n",
    "    with open(file, encoding=\"utf-8\") as f:\n",
    "        text = f.read()  \n",
    "        return text\n",
    "\n",
    "def count_sentences(files):\n",
    "    total = 0\n",
    "    for file in files:\n",
    "        content = read_file(file)\n",
    "        total += len([sent for sent in sent_tokenize(content) if len(sent) > MIN_SENT_LEN])\n",
    "    return total           \n",
    "\n",
    "expace_files = [EXPACE_DIR + file for file in listdir(EXPACE_DIR) if '.txt' in file]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-testimony",
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPER_PARAMS = {\n",
    "    'epochs': 3,\n",
    "    'lr': 3e-5,\n",
    "    'batch_size': 8,\n",
    "    'name': 'gpt2-expace-v1',\n",
    "}\n",
    "VAL_CHECK_INTERVAL = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-ghost",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-bridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from nltk import sent_tokenize\n",
    "from random import shuffle\n",
    "import re\n",
    "import string\n",
    "\n",
    "REMOVE_CIT_MATH = False\n",
    "REMOVE_NON_ASCII = False\n",
    "MIN_WORDS = 10\n",
    "USE_STRICT_PUNCT = False\n",
    "STRICT_PUNCT = '*+,-./%\\'\"!~'\n",
    "\n",
    "def valid_sentence(sentence):\n",
    "    if USE_STRICT_PUNCT:\n",
    "        pattern = f'{string.digits+string.ascii_letters+STRICT_PUNCT+string.whitespace}'\n",
    "    else:\n",
    "        pattern = f'{string.digits+string.ascii_letters+string.punctuation+string.whitespace}'\n",
    "    return not re.search(fr'[^{pattern}]', sentence)\n",
    "\n",
    "def load_sentences(datasets, files, max_sentences='all'):\n",
    "    for file in files:\n",
    "        content = read_file(file)\n",
    "        sentences = sent_tokenize(content)\n",
    "        \n",
    "        shuffle(sentences)\n",
    "        \n",
    "        counter_add = 0\n",
    "        \n",
    "        for sentence in sentences:  \n",
    "            if REMOVE_NON_ASCII and not valid_sentence(sentence):\n",
    "                continue\n",
    "            if len(sentence.split()) < MIN_WORDS:\n",
    "                continue\n",
    "                \n",
    "            if REMOVE_CIT_MATH:\n",
    "                if '[' in sentence or ']' in sentence:\n",
    "                    continue\n",
    "\n",
    "            if '\\n' in sentence:\n",
    "                continue                \n",
    "            datasets.append(sentence) \n",
    "            counter_add += 1\n",
    "            if max_sentences !='all' and max_sentences and counter_add >= max_sentences:\n",
    "                break\n",
    "                \n",
    "def save_dataset(dataset, name):\n",
    "    with open(name, 'w', encoding='UTF-8') as f:\n",
    "        for item in dataset:\n",
    "            f.write(f'{item}\\n')\n",
    "\n",
    "def build_preloaded_files(files, num_files, num_sents_per_file, output_folder, version_name):\n",
    "    datasets = []\n",
    "    \n",
    "    shuffle(files)\n",
    "    \n",
    "    print('Loading sentences')\n",
    "    if num_files == 'all':\n",
    "        load_sentences(datasets, files, num_sents_per_file)\n",
    "    else:\n",
    "        load_sentences(datasets, files[:num_files], num_sents_per_file)        \n",
    "\n",
    "    shuffle(datasets)\n",
    "\n",
    "    print ('Total sentences', len(datasets))\n",
    "    train_size = int(0.8 * len(datasets))\n",
    "    val_size = int(0.1 * len(datasets))\n",
    "    test_size = len(datasets) - val_size - train_size\n",
    "    \n",
    "    val_limit = train_size + val_size\n",
    "    train_data_with_label = datasets[0:train_size]\n",
    "    val_data_with_label = datasets[train_size:val_limit]\n",
    "    test_data_with_label = datasets[val_limit:]\n",
    "\n",
    "    save_dataset(train_data_with_label, f'{output_folder}/train-{num_files}-{num_sents_per_file}-{version_name}.txt')\n",
    "    save_dataset(val_data_with_label, f'{output_folder}/val-{num_files}-{num_sents_per_file}-{version_name}.txt')\n",
    "    save_dataset(test_data_with_label, f'{output_folder}/test-{num_files}-{num_sents_per_file}-{version_name}.txt')\n",
    "    return train_data_with_label, val_data_with_label, test_data_with_label                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FILES = 'all'\n",
    "SENTENCES_PER_FILE = 'all'\n",
    "DATA_VERSION = 'v1'\n",
    "train_data_with_label, val_data_with_label, test_data_with_label = build_preloaded_files(expace_files, NUM_FILES, SENTENCES_PER_FILE, f'{DATA_DIR}expace-sentences', DATA_VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-empty",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "class LazyLoadFileLines(IterableDataset):\n",
    "    def __init__(self, file):\n",
    "        with open(file, encoding=\"utf-8\") as f:\n",
    "            lines = f.read().rstrip().splitlines()\n",
    "        self.sentences = lines\n",
    "        self.length = len(lines) \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def get_next_sentence(self):    \n",
    "        for sent_index, sent in enumerate(self.sentences):\n",
    "            yield self.tokenize(sent, sent_index)\n",
    "    \n",
    "    def tokenize(self, sentence, sent_index):\n",
    "        encodings = tokenizer([sentence], truncation=True, padding='max_length', max_length=150)\n",
    "        item = {key: torch.tensor(val[0], device='cuda') for key, val in encodings.items()}\n",
    "        return item\n",
    "    \n",
    "    def collate(self, batch):\n",
    "        print (len(batch))\n",
    "        return batch\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self.get_next_sentence()\n",
    "    \n",
    "def get_iteratable_dataset_lines(num_files, num_sents_per_file, base_folder, version_name):\n",
    "    train_dataset = LazyLoadFileLines(f'{base_folder}train-{num_files}-{num_sents_per_file}-{version_name}.txt')\n",
    "    val_dataset = LazyLoadFileLines(f'{base_folder}/val-{num_files}-{num_sents_per_file}-{version_name}.txt')\n",
    "    test_dataset = LazyLoadFileLines(f'{base_folder}/test-{num_files}-{num_sents_per_file}-{version_name}.txt')\n",
    "\n",
    "    file_name = f'train-{num_files}-{num_sents_per_file}-{version_name}.txt'\n",
    "    return train_dataset, val_dataset, test_dataset, file_name\n",
    "\n",
    "NUM_FILES = 'all'\n",
    "SENTENCES_PER_FILE = 'all'\n",
    "DATA_VERSION = 'v1'\n",
    "\n",
    "train_dataset, val_dataset, test_dataset, file_name = get_iteratable_dataset_lines(NUM_FILES, SENTENCES_PER_FILE, f'{DATA)DIR}expace-sentences/', DATA_VERSION)\n",
    "\n",
    "HYPER_PARAMS['file'] = file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-camera",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = HYPER_PARAMS['batch_size']\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size \n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size \n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-constitutional",
   "metadata": {},
   "source": [
    "# Define trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-athens",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning.metrics import functional as FM\n",
    "from transformers import AdamW\n",
    "\n",
    "class LMHeadModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.valid_acc = pl.metrics.Accuracy()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.model.eval()\n",
    "        self.model.cuda()\n",
    "        \n",
    "        input_ids = tokenizer.encode(x, return_tensors='pt').to('cuda')\n",
    "        outputs = self.model(input_ids)\n",
    "        prob = F.softmax(outputs.logits.detach(), dim=1).cpu().numpy()[0].tolist()\n",
    "        \n",
    "        return {label: prob[index]  for index, label in enumerate(self.labels)}\n",
    "            \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        labels = batch[\"input_ids\"]\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        token_type_ids = None\n",
    "\n",
    "        outputs = self.model(input_ids=input_ids, labels=labels, attention_mask=batch[\"attention_mask\"])\n",
    "        \n",
    "        self.log('train_loss', outputs.loss)\n",
    "        return outputs.loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):  \n",
    "        outputs = self.model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"input_ids\"]\n",
    "        )\n",
    "         \n",
    "        self.log('validation_loss', outputs.loss)        \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.model.parameters(), lr = HYPER_PARAMS['lr'], eps = 1e-8)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-disaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "from pytorch_lightning.loggers import WandbLogger \n",
    "\n",
    "wandb.login()\n",
    "wandb_logger = WandbLogger() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-minority",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project='GPT2 - Expace')\n",
    "wandb.config.update(HYPER_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training model with:')\n",
    "print(json.dumps(HYPER_PARAMS,indent=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-institute",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_expace = LMHeadModel(model, tokenizer)\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=HYPER_PARAMS['epochs'], val_check_interval=VAL_CHECK_INTERVAL, checkpoint_callback=False, logger=wandb_logger)\n",
    "trainer.fit(gpt2_expace, train_dataloader, validation_dataloader)\n",
    "model.save_pretrained(f'{MODELS_DIR}{HYPER_PARAMS[\"name\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-avenue",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f'{MODELS_DIR}gpt2-expace-v1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
