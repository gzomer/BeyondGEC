{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "starting-september",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from random import shuffle\n",
    "import time\n",
    "import tqdm\n",
    "import json\n",
    "import hashlib\n",
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from os.path import exists\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import gc\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "automated-norway",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '' # Working dir\n",
    "DATA_DIR = f'{BASE_DIR}data/'\n",
    "MODELS_DIR = f'{BASE_DIR}models/'\n",
    "RESULTS_DIR = f'{BASE_DIR}results/'\n",
    "MODEL_DATA_DIR = f'{DATA_DIR}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "measured-constant",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "undefined-devon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "from transformers import T5Tokenizer\n",
    "import torch\n",
    "\n",
    "device = torch.device(DEVICE)\n",
    "\n",
    "IMPROVE_TOKEN = \"improve_english: \"\n",
    "IMPROVE_TOKEN_MULTI = \"improve_english\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-trinity",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "backed-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper:\n",
    "        \n",
    "    def __init__(self, tokenizer, config={}):\n",
    "        self.device = DEVICE\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "        self.vocab = tokenizer.get_vocab().keys()\n",
    "        \n",
    "    def encode_sentence(self, sent):\n",
    "        sent_special, map_tokens = self.add_special_tokens(sent)\n",
    "        inputs_ids = self.tokenizer.encode(sent_special, return_tensors='pt').to(device=device)\n",
    "\n",
    "        return inputs_ids, map_tokens\n",
    "\n",
    "    def get_special_sentences(self, sentences):\n",
    "        list_map_tokens = []\n",
    "        special_sentences = []\n",
    "        for sent in sentences:\n",
    "            sent_special, map_tokens = self.add_special_tokens(sent)\n",
    "            special_sentences.append(sent_special)\n",
    "            list_map_tokens.append(map_tokens)\n",
    "        \n",
    "        return special_sentences, list_map_tokens\n",
    "    \n",
    "    def encode_batch_inference(self, sentences):        \n",
    "        special_sentences, map_tokens = self.get_special_sentences(sentences)\n",
    "        encodings_input = self.tokenizer(special_sentences, truncation=True, padding='longest', return_tensors='pt').to(self.device)\n",
    "        \n",
    "        return encodings_input, map_tokens\n",
    "    \n",
    "    def decode_batch(self, outputs, map_tokens, original_sentences):        \n",
    "        return [self.decode_sentence(output, map_token, original) for output, map_token, original in zip(outputs, map_tokens, original_sentences) ]\n",
    "    \n",
    "    def encode_batch_training(self, sentences):        \n",
    "        special_sentences, map_tokens = self.get_special_sentences(sentences)\n",
    "        encodings_input = self.tokenizer(special_sentences, truncation=True, padding='longest')\n",
    "        \n",
    "        return encodings_input, map_tokens\n",
    "    \n",
    "    def decode_sentence(self, output, map_tokens, original=None):\n",
    "        sentence = self.tokenizer.decode(output, skip_special_tokens=True)\n",
    "        return self.replace_special_tokens(sentence, map_tokens, original)\n",
    "    \n",
    "    def in_vocab(self, char):\n",
    "        return char in self.vocab \n",
    "\n",
    "    def has_oov(self, sent):\n",
    "        chars = list(set([char for char in sent if char != ' ']))\n",
    "        for char in chars:\n",
    "            if not in_vocab(char, self.vocab):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def add_special_tokens(self, sent):\n",
    "        start_index = next((int(item.replace('[', '').replace(']','')) for item in sorted(re.findall(r'(\\[[0-9]+\\])',sent), reverse=True)), 0)\n",
    "        keep_to_tokens = {}\n",
    "\n",
    "        def map_char_to_token(char):\n",
    "            nonlocal start_index\n",
    "            start_index += 1\n",
    "            return f'[KEEP{start_index}]'\n",
    "\n",
    "        def convert_token(token):    \n",
    "            nonlocal keep_to_tokens\n",
    "            chars = list(token)\n",
    "            for char in chars:\n",
    "                if not self.in_vocab(char):            \n",
    "                    mapped_token = map_char_to_token(char)\n",
    "                    last_char = chars[-1]\n",
    "                    if last_char == ',' or last_char == '.':\n",
    "                        keep_to_tokens[mapped_token] = token[:-1]\n",
    "                        return mapped_token + last_char\n",
    "                    else:\n",
    "                        keep_to_tokens[mapped_token] = token         \n",
    "                        return mapped_token\n",
    "\n",
    "            return token\n",
    "\n",
    "        updated_sent = ' '.join([convert_token(token) for token in sent.split()])    \n",
    "                \n",
    "        def keep_parenthesis(sent):\n",
    "            nonlocal start_index\n",
    "\n",
    "            while re.search('(\\(.*?\\))+', sent):\n",
    "                start_index += 1\n",
    "                mapped_token = f'[KEEP{start_index}]'\n",
    "                match = str(re.search('(\\(.*?\\))+', sent)[0])\n",
    "                sent = re.sub('(\\(.*?\\))+', mapped_token, sent, 1)     \n",
    "                keep_to_tokens[mapped_token] = match\n",
    "            return sent\n",
    "\n",
    "        if self.config.get('keep_parentheses'):\n",
    "            updated_sent = keep_parenthesis(updated_sent)\n",
    "\n",
    "        return updated_sent, keep_to_tokens\n",
    "\n",
    "    def replace_special_tokens(self, sentence, keep_to_tokens, original):\n",
    "        for key in keep_to_tokens:\n",
    "            if key not in sentence and original:\n",
    "                return original\n",
    "            sentence = sentence.replace(key, keep_to_tokens[key])\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "successful-proportion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "        \n",
    "def compare_improve(sentence, model, improve_token, num_outputs=1):\n",
    "    print(\"BEFORE:\", sentence)\n",
    "    outputs = generate(model, improve_token + sentence, num_outputs)\n",
    "    print(\"\\nAFTER :\\n-\", '\\n- '.join([sent.strip() for sent in outputs]))\n",
    "    \n",
    "    return outputs\n",
    "    \n",
    "def generate(model, start, num_outputs=1, length=100):\n",
    "    inputs_ids = tokenizer.encode(start, return_tensors='pt').to(device=device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs_ids, \n",
    "                                max_length=length, num_beams=5, \n",
    "                                num_return_sequences=num_outputs,\n",
    "                                early_stopping=False)    \n",
    "        return [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "            \n",
    "def generate_batch(model, tokenizer, sentences, num_outputs=1, length=150, use_wrapper=True):        \n",
    "    if use_wrapper:\n",
    "        encodings_input, map_tokens = tokenizer.encode_batch_inference(sentences)\n",
    "    else:\n",
    "        encodings_input = tokenizer(sentences, truncation=True, padding='longest', return_tensors='pt').to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(encodings_input['input_ids'], \n",
    "                                max_length=length, num_beams=5, \n",
    "                                num_return_sequences=num_outputs,\n",
    "                                early_stopping=False)   \n",
    "\n",
    "    if use_wrapper:\n",
    "        return tokenizer.decode_batch(outputs, map_tokens, sentences)\n",
    "    else:\n",
    "        return [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "\n",
    "def process_sentences(model, sentences, token_start=IMPROVE_TOKEN):\n",
    "    parsed_sentences = []\n",
    "    for sent in tqdm.tqdm(sentences):\n",
    "        \n",
    "        outputs = generate(model, token_start + sent, 1)\n",
    "\n",
    "        if (outputs[0] != sent):\n",
    "            parsed_sentences.append({'orig':sent, 'improved':outputs})\n",
    "        \n",
    "    return parsed_sentences\n",
    "\n",
    "def process_sentences_batch(model, tokenizer, sentences, token_start=IMPROVE_TOKEN, batch_size=32, output=None):\n",
    "    parsed_sentences = []\n",
    "    \n",
    "    batches = chunks(sentences, batch_size)\n",
    "    total = math.ceil(len(sentences)/batch_size)\n",
    "    \n",
    "    counter = 0\n",
    "    for batch in tqdm.tqdm(batches, total=total):\n",
    "        outputs = generate_batch(model, tokenizer, [token_start + sent for sent in batch], 1)\n",
    "    \n",
    "        joined_outputs = list(zip(batch, outputs))\n",
    "\n",
    "        for sent in joined_outputs:\n",
    "            if sent[0] != sent[1]:\n",
    "                parsed_sentences.append({'orig':sent[0], 'improved':[sent[1]]})\n",
    "        counter += 1       \n",
    "        if counter % 100 == 0 and output:\n",
    "            with open(output,'w') as f:\n",
    "                f.write(json.dumps(parsed_sentences, indent=4))\n",
    "        \n",
    "    return parsed_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-prefix",
   "metadata": {},
   "source": [
    "# Multi lingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hindu-spray",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def process_files(files, model, tokenizer, processed_folder, limit, token_start=IMPROVE_TOKEN, suffix=None, input_dir=None, batch_size=32):\n",
    "    for file in files:\n",
    "        print ('Currrent file', file, 'token', token_start, 'limit', limit, 'suffix', suffix)\n",
    "        with open(DATA_DIR + f'{input_dir}/{file}','r') as f:\n",
    "            sentences_to_process = json.loads(f.read())\n",
    "        \n",
    "        base_file = file.split('.')[0]\n",
    "        if suffix:\n",
    "            output_file = f'{processed_folder}{base_file}-{limit}-{suffix}.json'\n",
    "        else:\n",
    "            output_file = f'{processed_folder}{base_file}-{limit}.json'\n",
    "            \n",
    "        if os.path.exists(output_file):\n",
    "            print('Skipping already processed', file)\n",
    "            continue\n",
    "            \n",
    "        parsed_sentences = process_sentences_batch(model, tokenizer, sentences_to_process[:limit], token_start, batch_size=batch_size, output=output_file)\n",
    "    \n",
    "        with open(output_file,'w') as f:\n",
    "            f.write(json.dumps(parsed_sentences, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-economy",
   "metadata": {},
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "lyric-reward",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_config = {\n",
    "    't5sm-l1aware-multi-s260-v1': {\n",
    "        'size': 't5-small',\n",
    "        'languages': ['pt','es'],\n",
    "    },\n",
    "    't5lg-l1aware-multi-s260-v1': {\n",
    "        'size': 't5-base',\n",
    "        'languages': ['pt','es'],\n",
    "    },        \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-inspiration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "TOKENIZER_CONFIG = {\n",
    "    'keep_parentheses': False\n",
    "}\n",
    "\n",
    "MAX_SENTENCES = 20000\n",
    "BATCH_SIZE= 16\n",
    "\n",
    "models_list = [\n",
    "    't5sm-l1aware-multi-s260-v1',\n",
    "    't5lg-l1aware-multi-s260-v1',\n",
    "]\n",
    "\n",
    "to_process_files = [\n",
    "    'brace-v1.json',\n",
    "    'lace-v1.json',\n",
    "]\n",
    "\n",
    "language_files = {\n",
    "    'brace-v1.json': 'pt', \n",
    "    'lace-v1.json': 'es', \n",
    "}\n",
    "\n",
    "def process_multi_models(models_list):    \n",
    "    for MODEL_VERSION in models_list:\n",
    "        # Clean up \n",
    "        torch.cuda.empty_cache()  \n",
    "            \n",
    "        print ('Loading model', MODEL_VERSION)\n",
    "        if DEVICE == 'cpu':\n",
    "            model = T5ForConditionalGeneration.from_pretrained(MODELS_DIR+MODEL_VERSION).cpu().eval()\n",
    "        else:\n",
    "            model = T5ForConditionalGeneration.from_pretrained(MODELS_DIR+MODEL_VERSION).cuda(DEVICE).eval()\n",
    "        \n",
    "        print ('Model loaded', MODEL_VERSION)\n",
    "        \n",
    "        model_config = models_config[MODEL_VERSION]\n",
    "        MULTI_LANGUAGES = model_config['languages']\n",
    "        \n",
    "        print('Using tokenizer', model_config['size'])\n",
    "        tokenizer = T5Tokenizer.from_pretrained(model_config['size'])\n",
    "        tokenizer_wrapper= TokenizerWrapper(tokenizer, TOKENIZER_CONFIG)        \n",
    "\n",
    "        processed_folder = f'{RESULTS_DIR}{MODEL_VERSION}/processed/'\n",
    "        pathlib.Path(processed_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        language = MODEL_VERSION.split('-')[2]\n",
    "        if language == 'multi':\n",
    "            for language in MULTI_LANGUAGES:\n",
    "                token_start = f'{IMPROVE_TOKEN_MULTI} {language}: '\n",
    "                suffix = f'token-{language}'  \n",
    "    \n",
    "                # Only process same language\n",
    "                filtered_files = []\n",
    "                for file in to_process_files:\n",
    "                    if language_files[file] != language:\n",
    "                        continue\n",
    "                    filtered_files.append(file)\n",
    "                \n",
    "                try:\n",
    "                    process_files(filtered_files, model, tokenizer_wrapper, processed_folder, MAX_SENTENCES, token_start=token_start,suffix=suffix, input_dir=input_dir, batch_size=BATCH_SIZE)    \n",
    "                except Exception as e:\n",
    "                    print (e)\n",
    "                    pass\n",
    "        elif language == 'all':\n",
    "            token_start = f'{IMPROVE_TOKEN}'     \n",
    "            try:\n",
    "                process_files(to_process_files, model, tokenizer_wrapper, processed_folder, MAX_SENTENCES, token_start=token_start, input_dir=input_dir, batch_size=BATCH_SIZE)    \n",
    "            except Exception as e:\n",
    "                print (e)\n",
    "                pass\n",
    "\n",
    "        else:\n",
    "            token_start = f'{IMPROVE_TOKEN_MULTI} {language}: '     \n",
    "            try:\n",
    "                process_files(to_process_files, model, processed_folder, MAX_SENTENCES, token_start=token_start, input_dir=input_dir, batch_size=BATCH_SIZE)    \n",
    "            except Exception as e:\n",
    "                print (e)\n",
    "                pass\n",
    "            \n",
    "        # Clean up\n",
    "        del model, tokenizer, tokenizer_wrapper\n",
    "        gc.collect()\n",
    "        time.sleep(10)\n",
    "        torch.cuda.empty_cache() \n",
    "        \n",
    "process_multi_models(models_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
