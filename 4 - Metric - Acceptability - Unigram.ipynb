{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-fiber",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '' # Working directory\n",
    "MODELS_DIR = f'{BASE_DIR}models/' # Models directory\n",
    "EXPACE_DIR = '' # ExPACE corpus folder (available on suplementary data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from random import shuffle\n",
    "from collections import defaultdict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-infection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    with open(file, encoding=\"utf-8\") as f:\n",
    "        return f.read()              \n",
    "    \n",
    "expace_files = [EXPACE_DIR + file for file in listdir(EXPACE_DIR) if '.txt' in file]\n",
    "shuffle(expace_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-failure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "gpt2_tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper:\n",
    "        \n",
    "    def __init__(self, tokenizer, config={}):\n",
    "        self.device = 'cuda'\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "        self.vocab = tokenizer.get_vocab().keys()\n",
    "        \n",
    "    def encode_sentence(self, sent):\n",
    "        sent_special, map_tokens = self.add_special_tokens(sent)\n",
    "        inputs_ids = self.tokenizer.encode(sent_special, return_tensors='pt').to(device=device)\n",
    "\n",
    "        return inputs_ids, map_tokens\n",
    "\n",
    "    def get_special_sentences(self, sentences):\n",
    "        list_map_tokens = []\n",
    "        special_sentences = []\n",
    "        for sent in sentences:\n",
    "            sent_special, map_tokens = self.add_special_tokens(sent)\n",
    "            special_sentences.append(sent_special)\n",
    "            list_map_tokens.append(map_tokens)\n",
    "        \n",
    "        return special_sentences, list_map_tokens\n",
    "    \n",
    "    def tokenize(self, sentence):\n",
    "        self.get_special_sentences(sentences)\n",
    "        \n",
    "    def encode_batch_inference(self, sentences):        \n",
    "        special_sentences, map_tokens = self.get_special_sentences(sentences)\n",
    "        encodings_input = self.tokenizer(special_sentences, truncation=True, padding='longest', return_tensors='pt').to(self.device)\n",
    "        \n",
    "        return encodings_input, map_tokens\n",
    "    \n",
    "    def decode_batch(self, outputs, map_tokens, original_sentences):        \n",
    "        return [self.decode_sentence(output, map_token, original) for output, map_token, original in zip(outputs, map_tokens, original_sentences) ]\n",
    "    \n",
    "    def encode_batch_training(self, sentences):        \n",
    "        special_sentences, map_tokens = self.get_special_sentences(sentences)\n",
    "        encodings_input = self.tokenizer(special_sentences, truncation=True, padding='longest')\n",
    "        \n",
    "        return encodings_input, map_tokens\n",
    "    \n",
    "    def decode_sentence(self, output, map_tokens, original=None):\n",
    "        sentence = self.tokenizer.decode(output, skip_special_tokens=True)\n",
    "        return self.replace_special_tokens(sentence, map_tokens, original)\n",
    "    \n",
    "    def in_vocab(self, char):\n",
    "        return char in self.vocab \n",
    "\n",
    "    def has_oov(self, sent):\n",
    "        chars = list(set([char for char in sent if char != ' ']))\n",
    "        for char in chars:\n",
    "            if not in_vocab(char, self.vocab):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def add_special_tokens(self, sent):\n",
    "        start_index = next((int(item.replace('[', '').replace(']','')) for item in sorted(re.findall(r'(\\[[0-9]+\\])',sent), reverse=True)), 0)\n",
    "        keep_to_tokens = {}\n",
    "\n",
    "        def map_char_to_token(char):\n",
    "            nonlocal start_index\n",
    "            start_index += 1\n",
    "            return f'[KEEP{start_index}]'\n",
    "\n",
    "        def convert_token(token):    \n",
    "            nonlocal keep_to_tokens\n",
    "            chars = list(token)\n",
    "            for char in chars:\n",
    "                if not self.in_vocab(char):            \n",
    "                    mapped_token = map_char_to_token(char)\n",
    "                    last_char = chars[-1]\n",
    "                    if last_char == ',' or last_char == '.':\n",
    "                        keep_to_tokens[mapped_token] = token[:-1]\n",
    "                        return mapped_token + last_char\n",
    "                    else:\n",
    "                        keep_to_tokens[mapped_token] = token         \n",
    "                        return mapped_token\n",
    "\n",
    "            return token\n",
    "\n",
    "        updated_sent = ' '.join([convert_token(token) for token in sent.split()])    \n",
    "                \n",
    "        def keep_parenthesis(sent):\n",
    "            nonlocal start_index\n",
    "\n",
    "            while re.search('(\\(.*?\\))+', sent):\n",
    "                start_index += 1\n",
    "                mapped_token = f'[KEEP{start_index}]'\n",
    "                match = str(re.search('(\\(.*?\\))+', sent)[0])\n",
    "                sent = re.sub('(\\(.*?\\))+', mapped_token, sent, 1)     \n",
    "                keep_to_tokens[mapped_token] = match\n",
    "            return sent\n",
    "\n",
    "        if self.config.get('keep_parentheses'):\n",
    "            updated_sent = keep_parenthesis(updated_sent)\n",
    "\n",
    "        return updated_sent, keep_to_tokens\n",
    "\n",
    "    def replace_special_tokens(self, sentence, keep_to_tokens, original):\n",
    "        for key in keep_to_tokens:\n",
    "            if key not in sentence and original:\n",
    "                return original\n",
    "            sentence = sentence.replace(key, keep_to_tokens[key])\n",
    "        return sentence\n",
    "    \n",
    "tokenizer_wrapper = TokenizerWrapper(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-manner",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "from tqdm import tqdm\n",
    "from functools import reduce\n",
    "\n",
    "class UnigramModel():\n",
    "    def __init__(self, tokenizer):\n",
    "        self.unigram_model = defaultdict(int)\n",
    "        self.word_count = 0\n",
    "        self.vocab_count = 0\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def tokenize_subwords(self, sent):\n",
    "        # This is the special token use by GPT-2 to separate words\n",
    "        return [item.replace('Ä ', '') for item in self.tokenizer.tokenize(sent)]\n",
    "\n",
    "    def build(self, files, limit=None):\n",
    "        for file in tqdm(files[:limit]):\n",
    "            content = read_file(file)\n",
    "            for sent in sent_tokenize(content):\n",
    "                if len(sent) > 1024:\n",
    "                    continue\n",
    "                for word in self.tokenize_subwords(sent):\n",
    "                    token = word.lower()\n",
    "                    self.unigram_model[token] += 1\n",
    "           \n",
    "        self.word_count = sum(self.unigram_model.values())\n",
    "        self.vocab_count = len(self.unigram_model.keys())\n",
    "    \n",
    "    def prob(self, words):\n",
    "        probs = [self.prob_token(sub) for sub in self.tokenize_subwords(words)]\n",
    "        return reduce((lambda x, y: x * y), probs)\n",
    "        \n",
    "    def prob_token(self, token):\n",
    "        return (self.unigram_model[token]+1)/(self.word_count+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-price",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_model = UnigramModel(gpt2_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-passion",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_model.build(expace_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{MODELS_DIR}/unigram-expace/model-v2.pkl','wb') as f:\n",
    "    pickle.dump(uni_model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
